<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Recognize Anything: A Strong Image Tagging Model.">
  <meta name="keywords" content="RAM, Image Tagging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Recognize Anything: A Strong Image Tagging Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
    <!-- <link rel="icon" > -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22> </text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com"> -->
        <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://recognize-anything.github.io/">
            Recognize Anything
          </a>
          <a class="navbar-item" href="https://tag2text.github.io/">
            Tag2Text
          </a>
          <a class="navbar-item" href="https://github.com/xinyu1205/IDEA-pytorch">
            IDEA
          </a>
          <a class="navbar-item" href="https://github.com/xinyu1205/robust-loss-mlml">
            Robust-Loss-MLML
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Recognize Anything<br><font size="6.5">A Strong Image Tagging Model</font></h1>
          <!-- <h2 class="title is-1 publication-title"><font size="6">A Strong Image Tagging Model</font></h2> -->
          <!-- <h1 class="title is-1 publication-title">Recognize Anything<br> A Strong Image Tagging Model</h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=j5idDP8AAAAJ">Youcai Zhang</a><sup>1*</sup>,</span>
            <span class="author-block">
            <span class="author-block">
              <a href="https://xinyu1205.github.io/">Xinyu Huang</a><sup>1*</sup>,</span>
              <a>Jinyu Ma</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a>Zhaoyang Li</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a>Zhaochuan Luo</a><sup>1</sup>,</span>
              <span class="author-block">
                <a>Yanchun Xie</a><sup>1</sup>,</span><br>
                <span class="author-block">
                  <a>Yuzhuo Qin</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a>Tong Luo</a><sup>1</sup>,</span>
                <span class="author-block">
                <a>Yaqian Li</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.lsl.zone/">Shilong Liu</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=fWDoWsQAAAAJ">Yandong Guo</a><sup>3</sup>,</span>    
                  <span class="author-block">
                    <a href="http://www.leizhang.org/">Lei Zhang</a><sup>2</sup></span>                          

          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>*Equal Contribution</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>OPPO Research Institute,</span>
            <span class="author-block"><sup>2</sup>International Digital Economy Academy (IDEA),</span>
            <span class="author-block"><sup>3</sup>AI<sup>2</sup> Robotics</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2306.03514.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>RAM Paper</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/xinyu1205/Tag2Text"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Official Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://tag2text.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="fas fa-file-pdf"></i> -->
                      <i class="fab fa-slideshare"></i>
                  </span>
                  <span>Tag2Text</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-face-smiling-hands"></i>
                  </span>
                  <span>Tag2Text Demo</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Tag2Text & Grounded-SAM </span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-face-smiling-hands"></i>
                  </span>
                  <span>RAM Demo (Comming Soon!)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/ram_grounded_sam.jpg" style="width: 100%" alt>
      <h2 class="subtitle has-text-centered">
        <span style="color: orange; font-weight:bold"> The Recognize Anything Model (RAM) can recognize any common category with high accuracy.</span> 
       <br> When combined with <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank" title="tag_list">localization model (Grounded-SAM)</a>, RAM forms a strong and general pipeline for visual semantic analysis.
        </h2>
    </div>
  </div>
</section>




<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Highlight</h2>
          <div class="content has-text-justified">
          <p>
            <b>Recognition and localization are two foundation computer vision tasks.</b>
            <a href="https://segment-anything.com/">The Segment Anything Model (SAM)</a> excels in providing strong localization capabilities, while it falls short when it comes to recognition tasks. 
            <br>   In contrast, <b> the Recognize Anything Model (RAM) exhibits exceptional recognition abilities</b>, surpassing existing models in terms of both <b>accuracy and scope.</b><br>
               <div class="hero-body" style="text-align: center">
                <img src="static/images/localization_and_recognition.jpg" style="width: 80%; text-align: center" alt>
              </div>
            <!-- <img src="static/images/localization_and_recognition.jpg"  style="max-width:100%;max-height:100%;vertical-align: middle;   "  alt > -->
                  </p>
          </div>
        </div>
    </div>
    <!--/ Abstract. -->

    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width" >
      <!-- Visual Effects. -->
      <h2 class="title is-3">Recognition Results Comparison</h2>
      <div class="content has-text-justified" style="text-align: center;">
      <p>
        RAM can recognize <b>more valuable tags</b> than other models. <br>
        <li>RAM showcases <b>impressive zero-shot performance</b>, significantly <b>outperforming CLIP and BLIP</b>.</li>
        <li> RAM even surpasses the <b>fully supervised manners</b> (ML-Decoder) </li>
        <li> RAM exhibits competitive performance with the <b>Google tagging API</b>.</li>
          <img src="static/images/tagging_results.jpg" style="width: 100%; text-align: center" alt>
    </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <!-- Visual Effects. -->
      <h2 class="title is-3">Recognition Scopes Comparison</h2>
      <div class="content has-text-justified">
      <p>
        <li><b>RAM recognizes 6400+ common tags</b>, covering more valuable categories than OpenImages V6.</li>
        <li> With <b>open-set capability</b>, RAM is feasible to <b>recognize any common category</b>. </li>
        <div class="hero-body" style="text-align: center">
          <img src="static/images/category_distribution.jpg" style="width: 60%; text-align: center" alt>
        </div>
                <!-- <img src="static/images/category_distribution.jpg" style="width: 60%" alt> -->
    </div>
    </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@misc{zhang2023recognize,
title={Recognize Anything: A Strong Image Tagging Model}, 
author={Youcai Zhang and Xinyu Huang and Jinyu Ma and Zhaoyang Li and Zhaochuan Luo and Yanchun Xie and Yuzhuo Qin and Tong Luo and Yaqian Li and Shilong Liu and Yandong Guo and Lei Zhang},
year={2023},
eprint={2306.03514},
archivePrefix={arXiv},
primaryClass={cs.CV}

@article{huang2023tag2text,
  title={Tag2Text: Guiding Vision-Language Model via Image Tagging},
  author={Huang, Xinyu and Zhang, Youcai and Ma, Jinyu and Tian, Weiwei and Feng, Rui and Zhang, Yuejie and Li, Yaqian and Guo, Yandong and Zhang, Lei},
  journal={arXiv preprint arXiv:2303.05657},
  year={2023}}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2303.05657">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/xinyu1205/Tag2Text" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
